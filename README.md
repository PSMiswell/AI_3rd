# 딥 러닝 모델을 이용한 챗봇 구현

## 1 프로젝트 개요

> 텍스트 전 처리 기법과 머신러닝 기법을 응용하여 딥 러닝 기반의 챗봇을 구현합니다.
>
> 본 프로젝트에서는 실제 입력된 텍스트에 대응하는 자연어 대답을 만드는 챗봇을 개발하고 지금까지 배운 내용을 바탕으로 자신이 속한 팀만의 다양한 특성을 갖는 챗봇으로 확장합니다.
>
> Sub PJT3에서는 챗봇 구현에 핵심인 딥 러닝 기술을 구현하기 위하여 그 기반이 되는 Neural Network에 대하여 학습합니다.

### 1. 머신러닝 파트

> 머신 러닝 파트에서는 딥 러닝 기반 자연어 처리가 가능한 챗봇을 구현합니다. 머신 러닝의 구조는 5가지 파트로 분류할 수 있습니다.

1. 데이터를 읽기

   * ChatbotData.csv  파일을 읽고 트레이닝 데이터와 테스트 데이터로 저장합니다.
   * ChatbotData.csv 파일은 Q열에는 질문 문장, A 열에는 대답 문장이 저장되어 있습니다.

2. 문장 데이터 자연어 처리하기

   * 문장 데이터를 처리하기 위해서 자연어 처리 과정을 거쳐 컴퓨터가 처리할 수 있는 데이터로 변환합니다.
   * 읽어 온 문장 데이터는 `prepro_noise_canceling()`를 통하여 불필요한 성분을 제거하고 tokenizing_data()을 통하여 tokenizing합니다.
   * prepro_noise_canceling() 경우, 띄어쓰기 오류 및 스펠링 체크 기능을 추가하여 그 기능을 강화할 수 있습니다.
   * 사용하게 될 Language Model의 인코더와 디코더에 사용될 데이터 전 처리를 합니다.
   * `enc_processing()`은 인코더용 전 처리를 하는 함수로서 전 처리된 문장과 padding 처리가 되지 않은 문장의 길이를 반환합니다.
   * 디코더용 전 처리는 `dec_input_processing()`, `dec_target_processing()`로 두가지 함수를 사용합니다.
   * 각각 디코더의 입력으로 사용될 입력값을 만드는 함수와 디코더의 결과로 학습을 위해 필요한 라벨인 타깃값을 만드는 전 처리 함수입니다.

3. Language model을 사용하여 학습

   * Language model의 기본적인 구조인 Seq2seq 모델을 구현합니다.

     > **Seq2seq** : 시퀀스 형태의 입력 값을 시퀀스 형태의 출력으로 만들 수 있게 하는 모델로 하나의 텍스트 문장이 입력으로 들어오면 하나의 텍스트 문장을 출력하는 구조

   * RNN(LSTM) cell 을 기반으로 하며 모델은 크게 인코더 부분과 디코더 부분으로 나뉩니다. 먼저 인코더 부분에서 입력을 받아 입력 값의 정보를 담은 벡터를 만들어내고 디코더에서는 이 벡터를 활용해 재귀적으로 출력 값을 만들어 내는 구조입니다.

4. 학습된 결과물로 테스트 데이터를 통하여 정확도 측정

   * main.py를 통하여 전 처리된 데이터를 학습하고 evaluation metric 기법으로 주로 사용되는 BLEU 와 Rouge을 적용하여 정확도를 판별합니다.

5. 학습된 결과물을 저장

   * data_out 폴더에 저장된 사전 데이터(vocabularyData.voc)와 체크 포인트 폴더(check_point)의 정보를 이용하여 학습된 정보로 사용합니다.

### 2. Slack App 구현 파트

> 딥 러닝 기반의 자연어 처리가 가능한 챗봇의 Flask 서버를 통하여 구현합니다.
>
> 입력된 데이터를 바탕으로 SQLite를 사용하여 app.db를 생성합니다. 이번 프로젝트에서는 일차적으로 Slack에서 입력하는 챗봇 문장을 app.db에 저장하는 것을 목표로 합니다.



## 2 프로젝트 목표

#### 1. 딥 러닝 기반 자연어 처리가 가능한 챗봇 구현하기

1. 질문과 대답으로 이루어진 데이터를 읽고 트레이닝 데이터와 테스트 데이터로 저장하기
2. 문장 데이터를 Noise Canceling, Tokenizing 하기
3. token들이 저장된 단어 사전 vocabularyData.voc 파일 만들기
4. 인코딩용, 디코딩용 임베딩 함수 만들기
5. Seq2seq 모델을 불러와 학습하기
6. 정확도 평가를 위한 BLEU, Rouge Score 구하기
7. predict.py를 구현하여 새로운 질문에 대한 대답 데이터 출력하기
8. Python Slack Client를 사용하여 Flask 서버 app.py 구현하기
9. Slack App 에서 새로운 질문에 대한 대답 출력하기
10. SQLite를 사용해서 slack에 입력받을 데이터를 저장할 app.db만들기
11. app.db에 새로운 데이터 업데이트 하기



## 3 기능 명세

### 1. 딥 러닝 파트

#### Req. 1-1 트레이닝 데이터 전 처리(data.py)

1. 질의 응답 데이터 읽기 및 질문과 답변 데이터 분류 후, 테스트 데이터와 트레이닝 데이터로 나누어 저장하는 함수 load_data() 구현

   > `Output`
   >
   > train_q : 트레이닝용 질문 데이터
   >
   > train_a : 트레이닝용 대답 데이터
   >
   > test_q : 테스트용 질문 데이터
   >
   > test_a : 테스트용 대답 데이터

2. 텍스트 데이터에 정규화를 사용하여 `([~,.!?|"':;)(])` 제거하는 함수 prepro_noise_canceling() 구현

   > `Input`
   >
   > 텍스트 데이터
   >
   > `Output`
   >
   >  [~,.!?|"':;)(]) 제거된 텍스트 데이터

3. 텍스트 데이터들을 토크나이징 하는 함수 tokenizing_data() 구현

   > `Input`
   >
   > 텍스트 데이터
   >
   > `Output`
   >
   > [token1, token2, ...]
   >
   > 다음 내용들이 적용되어야 함(알고리즘 순서)
   >
   > 1) 텍스트 데이터 prepro_noise_canceling() 함수 처리
   >
   > 2) 띄어쓰기 단위로 나누기
   >
   > 3) 띄어진 단어들로 벡터 형성

#### Req. 1-2 인코더 디코더 입력용 데이터 전 처리(data.py)

1. 토큰화된 트레이닝 데이터를 인코더에 활용할 수 있도록 전 처리 함수 enc_processing() 구현

   > `Input`
   >
   > value : 텍스트 문장들 데이터
   >
   > dictionary : 값이 인덱스인 단어 사전
   >
   > `Output`
   >
   > 넘파이 문장 인덱스 벡터
   >
   > 다음 내용들이 적용되어야 함
   >
   > 1) 텍스트 데이터 prepro_noise_canceling() 함수 처리
   >
   > 2) 문장을 토큰 단위로 나누기
   >
   > 3) dictionary 를 활용하여 토큰 인덱스화
   >
   > 4) dictionary 에 없는 토큰의 경우 UNK 값으로 대체
   >
   > 5) 기준 문장 길이보다 크게 된다면 뒤의 토큰 자르기
   >
   > 6) 기준 문장 길이에 맞게 남은 공간이 있다면 padding 하기
   >
   > 7) 문장 인덱스와 문장 길이 계산

2. 디코더 입력부에 필요한 전 처리 함수 dec_output_processing()구현

   > `Input`
   >
   > value : 텍스트 문장들 데이터
   >
   > dictionary : 값이 인덱스인 단어 사전
   >
   > `Output`
   >
   > 넘파이 문장 인덱스 벡터
   >
   > 1) 텍스트 데이터 prepro_noixe_canceling() 함수 처리
   >
   > 2) 문장을 토큰 단위로 나누기
   >
   > 3) dictionary를 활용하여 토큰 인덱스화
   >
   > 4) 첫 인덱스에 STD추가
   >
   > 5) dictionary에 없는 토큰의 경우 UNk 값으로 대체
   >
   > 6) 기준 문장 길이 보다 크게 된다면 뒤의 토큰 자르기
   >
   > 7) 기준 문장 길이에 맞게 남은 공간이 있다면 padding 하기
   >
   > 8) 문장 인덱스와 문장 길이 계산

3. 디코더에 출력부에 필요한 데이터 전 처리 함수 dec_target_processing()구현

   > `Input`
   >
   > value : 텍스트 문장들 데이터
   > dictionary : 값이 인덱스인 단어 사전
   >
   > `Output`
   >
   > 넘파이 문장 인덱스 벡터
   >
   > 1) 텍스트 데이터 prepro_noise_canceling()함수 처리
   >
   > 2) 문장을 토큰 단위로 나누기
   >
   > 3) dictionary를 활용하여 토큰 인덱스화
   >
   > 4) dictionary에 없는 토큰의 경우 UNK 값으로 대체
   >
   > 5) 기준 문장 길이 보다 크게 된다면 뒤의 토큰 자르기
   >
   > 6) 기준 문장 길이에 맞게 남은 공간이 있다면 padding 하기
   >
   > 7) 마지막 인덱스에 END추가 (기준 문장 길이를 넘지 않음)
   >
   > 8) 문장 인덱스와 문장 길이 계산

#### Req. 1-3 : 사전 데이터 형성(data.py)

1. load_voc()함수 구현

   > 단어 사전 파일 vocabularyData.voc를 생성하고 단어와 인덱스 관계를 출력하는 load_voc()함수 구현 
   >
   > `Output`
   >
   > {token1:index1, token2:index2, ...}, {index1:token1, index2:token:2,...}, length of voc

2. make_voc()함수 구현

   > 사전 리스트를 받아 인덱스와 토큰의 dictionary를 생성하는 make_voc()함수 구현
   >
   > `Output`
   >
   > {token1:index1, token2:index2, ...}, {index1:token1, index2:token:2,...}

3. pred_next_string()함수 구현

   >  예측용 단어 인덱스를 문장으로 변환하는 pred_next_string()함수 구현
   >
   > `Input`
   >
   > value : 텍스트 문장들 데이터
   > dictionary : 값이 인덱스인 단어 사전
   >
   > `Output`
   >
   > 변환된 문장, 출력 완료 Boolean값

#### Req. 1-4 : Seq2seq 모델 및 파라미터 구성(model.py, config.py)
1. 작성된 model.py를 이해하고 config.py에서의 파라미터 값을 조정해가며 성능을 비교

#### Req. 1-5 : 모델 트레이닝 및 정확도 측정(main.py)
1. 자연어 처리된 결과물의 정확성을 판단하기 위하여 BLEU SCORE를 계산하는 bleu_compute()함수 구현

   > `Input`
   >
   > 실제 대답 문장, 예측된 대답 문장
   >
   > `Output`
   >
   > BLEU SCORE

2. 자연어 처리된 결과물의 정확성을 판단하기 위하여 ROUGE SCORE를 계산하는 bleu_compute()함수 구현

   > `Input`
   >
   > 실제 대답 문장, 예측된 대답 문장
   >
   > `Output`
   >
   > ROUGE SCORE (rouge-1의 r값 p값 f값을 출력)

3. 모델 학습에 필요한 data.py, config.py, model.py기능이 들어간 메인 함수 구현 및 트레이닝 구현으로 data_out폴더 데이터 생성

   > 실행 예시 : CPU만으로 학습 시 약 5~6시간, GPU 사용 시 약 2시간 소요 중간 중간 체크포인트가 생성되며 최적화가 되지 않아도 predict.py에 사용 가능

#### Req. 1-6 : 새로운 입력에 따른 대답 텍스트 출력(predict.py)
1. 학습된 단어 사전을 바탕으로 새로운 데이터를 전 처리하고 예측 대답을 출력하는 predict.py구현

> 실행 예시 : 명령 프롬프트 창에서 프롲게트 파일 경로를 설정 후 `python predict.py`입력



### 2. 웹 구현 및 데이터베이스 파트

#### Req. 2-1 : 웹에서 주고받는 데이터를 저장할 데이터베이스 파일 초기화
1. SQLite를 이용하여 새롭게 입력 받을 데이터를 저장할 DB파일 생성

2. db_init.py를 사용하여 텍스트 데이터를 저장할 app.db 파일 생성 (추가적으로 입력 정보 및 예측된 답변 데이터 까지 저장할 수 있는 DB 파일 설정 가능)


#### Req. 2-2 : 웹 서버 구현을 위한 Flask 서버 구현

1. 문장 데이터를 받아 예측된 대답 문장을 출력하는 predict() 함수 구현

   > `Input`
   >
   > 문장 데이터
   >
   > `Output`
   >
   > 대답 문장

2. app.db를 연동하여 웹에서 주고받는 데이터를 DB로 저장


#### Req. 2-3 : 질문 데이터를 입력하고 예상 대답 출력

1. Slack app 등록된 워크스페이스에서 app을 호출하여 문장 데이터를 입력

   > 자유로운 방식으로 문장 데이터를 받는 부분 구현

2. Flask에서 학습된 모델 기반으로 분류된 데이터 출력

   >  자유로운 방식으로 출력 방식 구현

3. Slack App을 이용하여 딥 러닝 기반 챗봇을 웹에서 구현

   >  예시) 문장 텍스트 입력 -> 대답 문장 출력 받기
   > \+ 입력된 데이터로 DB 업데이트

4. 1가지 이상의 새로운 기능을 추가한 챗봇 구현

   > 1. 출력된 답변이 맞지 않는 경우를 고려하여 정확한 답변을 새롭게 입력 받을 수 있는 기능
   > 2. 입력데이터 외에 챗봇의 답변 데이터 또한 저장하여 새로운 데이터 셋을 구성
   > 3. 웹 크롤링을 바탕으로 새로운 데이터 셋을 만들어 데이터 셋 업데이트